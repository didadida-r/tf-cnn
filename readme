tf-cnn1 timit
This project is testing the tf-kaldi nnet

20170510
1. 修改了学习率策略
2. run.sh用于提取fbank的fmllr特征，再run_cnn.sh中进行变换输入网络


较好的实验结果
1.
由于显存不足，没有使用动态特征
与tf-dnn4相比，修改了学习率策略
fbank feature + d +dd :40 no energy 
使用fbank + raw-fmllr
context=11
1 conv  [9, 9, 1, 128] stride=[1, 1, 1, 1]
1 pool  [1, 3, 1, 1] 
1 conv  [4, 3, 128, 256] stride=[1, 1, 1, 1]
2 FL 1000 units（1 hidden & softmax）
对齐使用tri3：
wer:25.0  frame acc: 47.6

同样的配置，epoch增到25
wer:   frame acc:  

2.
















###############
1.
test wer:21.1
dev wer:19.4
config: 

fmllr without deltas:

inputs_img = tf.reshape(inputs, tf.stack( [ tf.shape(inputs)[0] , 40, 1, 11] )  ) 
inputs_img = tf.transpose(inputs_img, [ 0 , 1, 3, 2 ] )  # N*36*9*3

conv1 = self.convolution(inputs_img, 'conv_l1', [9, 7, 1, 256], reuse, is_training)
pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 1, 1], strides=[1, 3, 1, 1], padding='VALID')
conv2 = self.convolution(pool1, 'conv_2', [4, 5, 256, 256], reuse, is_training)

2. 
###############