# Running on compute-0-3
# Started at Sat Jun 3 10:41:20 CST 2017
# dnn.sh 
Sat Jun  3 10:41:20 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K10.G2.8GB    On   | 0000:06:00.0     Off |                    0 |
| N/A   34C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K10.G2.8GB    On   | 0000:07:00.0     Off |                    0 |
| N/A   28C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K10.G2.8GB    On   | 0000:0A:00.0     Off |                    0 |
| N/A   29C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K10.G2.8GB    On   | 0000:0B:00.0     Off |                    0 |
| N/A   30C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   4  Tesla K10.G2.8GB    On   | 0000:87:00.0     Off |                    0 |
| N/A   32C    P8    18W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   5  Tesla K10.G2.8GB    On   | 0000:88:00.0     Off |                    0 |
| N/A   30C    P8    18W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   6  Tesla K10.G2.8GB    On   | 0000:8B:00.0     Off |                    0 |
| N/A   32C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   7  Tesla K10.G2.8GB    On   | 0000:8C:00.0     Off |                    0 |
| N/A   29C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   8  Tesla K10.G2.8GB    On   | 0000:91:00.0     Off |                    0 |
| N/A   31C    P8    19W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|   9  Tesla K10.G2.8GB    On   | 0000:92:00.0     Off |                    0 |
| N/A   27C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|  10  Tesla K10.G2.8GB    On   | 0000:95:00.0     Off |                    0 |
| N/A   31C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
|  11  Tesla K10.G2.8GB    On   | 0000:96:00.0     Off |                    0 |
| N/A   28C    P8    17W / 117W |      0MiB /  3527MiB |      0%   E. Process |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
export CUDA_VISIBLE_DEVICES=0
the input dim is:40
the output labels is:1947
the utt's maxlength is: 777
the total frame in training set is: 1124823
Use the lstm type 3
And we will pad the max-length of all utt to be divisible by 20
------- get alignments ----------
copy-int-vector 'ark:gunzip -c tf-exp/cnn/ali.binary.gz |' ark:- 
ali-to-pdf exp/tri3_ali/final.mdl ark:- ark,t:- 
LOG (copy-int-vector:main():copy-int-vector.cc:83) Copied 3696 vectors of int32.
LOG (ali-to-pdf:main():ali-to-pdf.cc:68) Converted 3696 alignments to pdf sequences.
------- training neural net ----------
------The LSTM Config------
Type3: The lstm data is processed in sub-seq
the inputs data is: 
List with len: 780 each element is 2-D tensor: (2, 40)
the tensorflow version is:1.1.0
the total sub-seq num is: 39
not use the former sub-seq output state
(20, 78, 40)
final_inputs is: 20
the lstm outputs is: 
List with len: 780 each element is 2-D tensor: (2, 512)
test
Tensor("input_seq_length:0", shape=(2,), dtype=int32)
------The DNN Config------
use 0 FL hidden layer
------The LSTM Config------
Type3: The lstm data is processed in sub-seq
the inputs data is: 
List with len: 780 each element is 2-D tensor: (2, 40)
the tensorflow version is:1.1.0
the total sub-seq num is: 39
not use the former sub-seq output state
(20, 78, 40)
final_inputs is: 20
the lstm outputs is: 
List with len: 780 each element is 2-D tensor: (2, 512)
test
Tensor("input_seq_length:0", shape=(2,), dtype=int32)
------The DNN Config------
use 0 FL hidden layer
/home/xiaorong/dev/python3.4/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
2017-06-03 10:41:33.133612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-03 10:41:33.133650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-03 10:41:33.133661: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-03 10:41:33.515515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K10.G2.8GB
major: 3 minor: 0 memoryClockRate (GHz) 0.745
pciBusID 0000:06:00.0
Total memory: 3.44GiB
Free memory: 3.41GiB
2017-06-03 10:41:33.515922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-03 10:41:33.515945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-03 10:41:33.515990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K10.G2.8GB, pci bus id: 0000:06:00.0)
2017-06-03 10:41:34.763264: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1596 get requests, put_count=1209 evicted_count=1000 eviction_rate=0.82713 and unsatisfied allocation rate=0.931704
2017-06-03 10:41:34.763401: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2017-06-03 10:41:35.797469: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1606 get requests, put_count=1543 evicted_count=1000 eviction_rate=0.648088 and unsatisfied allocation rate=0.676214
2017-06-03 10:41:35.797532: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
2017-06-03 10:41:37.965362: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5621 get requests, put_count=5672 evicted_count=1000 eviction_rate=0.176305 and unsatisfied allocation rate=0.179328
2017-06-03 10:41:37.965438: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
validation loss at step 0: 7.573110, acc is 0.014273
step 0/346 loss: 7.573255
step 1/346 loss: 7.538713
step 2/346 loss: 10.104456
step 3/346 loss: 7.529613
step 4/346 loss: 7.531188
step 5/346 loss: 7.503005
step 6/346 loss: 7.530910
step 7/346 loss: 7.489305
step 8/346 loss: 7.489185
step 9/346 loss: 7.521056
validation loss at step 10: 7.462667, the acc is: 0.038685
step 10/346 loss: 7.456132
step 11/346 loss: 7.472164
step 12/346 loss: 7.467397
step 13/346 loss: 7.413149
step 14/346 loss: 7.420580
step 15/346 loss: 7.375046
step 16/346 loss: 7.391987
step 17/346 loss: 7.338944
step 18/346 loss: 7.305039
step 19/346 loss: 7.272276
validation loss at step 20: 7.300766, the acc is: 0.036289
step 20/346 loss: 7.261458
step 21/346 loss: 7.249530
step 22/346 loss: 7.215251
step 23/346 loss: 7.157087
step 24/346 loss: 7.137534
step 25/346 loss: 7.056724
step 26/346 loss: 7.139041
step 27/346 loss: 6.997522
step 28/346 loss: 7.060493
step 29/346 loss: 7.065405
validation loss at step 30: 7.061913, the acc is: 0.046270
step 30/346 loss: 7.038458
step 31/346 loss: 7.074626
step 32/346 loss: 6.919464
step 33/346 loss: 6.907690
step 34/346 loss: 7.027155
step 35/346 loss: 7.024977
step 36/346 loss: 6.965916
step 37/346 loss: 7.014546
step 38/346 loss: 7.012847
step 39/346 loss: 6.952990
validation loss at step 40: 7.013941, the acc is: 0.046691
step 40/346 loss: 6.981168
step 41/346 loss: 6.905453
step 42/346 loss: 6.970390
step 43/346 loss: 7.051580
step 44/346 loss: 6.974578
step 45/346 loss: 6.982510
step 46/346 loss: 6.969575
step 47/346 loss: 6.980923
step 48/346 loss: 6.981068
step 49/346 loss: 6.938130
validation loss at step 50: 6.970201, the acc is: 0.045164
step 50/346 loss: 6.934320
step 51/346 loss: 6.921693
step 52/346 loss: 6.968400
the epoch 1 has been processed
step 53/346 loss: 6.887102
step 54/346 loss: 6.922406
step 55/346 loss: 6.955493
step 56/346 loss: 6.893698
step 57/346 loss: 6.942519
step 58/346 loss: 6.918085
step 59/346 loss: 6.816490
validation loss at step 60: 6.936262, the acc is: 0.044584
step 60/346 loss: 6.971302
step 61/346 loss: 6.810004
step 62/346 loss: 6.905503
step 63/346 loss: 6.888496
step 64/346 loss: 6.915668
step 65/346 loss: 6.894576
step 66/346 loss: 6.929506
step 67/346 loss: 6.884126
step 68/346 loss: 6.990259
step 69/346 loss: 6.916939
validation loss at step 70: 6.921072, the acc is: 0.045032
step 70/346 loss: 6.887977
step 71/346 loss: 6.915899
step 72/346 loss: 6.907999
step 73/346 loss: 6.878108
step 74/346 loss: 6.775223
step 75/346 loss: 6.893698
step 76/346 loss: 6.893062
step 77/346 loss: 6.895543
step 78/346 loss: 6.897230
step 79/346 loss: 6.889772
validation loss at step 80: 6.909243, the acc is: 0.045269
step 80/346 loss: 6.896104
step 81/346 loss: 6.862956
step 82/346 loss: 6.851119
step 83/346 loss: 6.873439
step 84/346 loss: 6.932925
step 85/346 loss: 6.869833
step 86/346 loss: 6.924352
step 87/346 loss: 6.807551
step 88/346 loss: 6.802887
step 89/346 loss: 6.809615
validation loss at step 90: 6.902430, the acc is: 0.045111
step 90/346 loss: 6.878741
step 91/346 loss: 6.853159
step 92/346 loss: 6.892946
step 93/346 loss: 6.864452
step 94/346 loss: 6.870884
step 95/346 loss: 6.794120
step 96/346 loss: 6.893785
step 97/346 loss: 6.788173
step 98/346 loss: 6.881556
step 99/346 loss: 6.927467
validation loss at step 100: 6.896654, the acc is: 0.045006
