[directories]
prjdir = /home/xiaorong/workstation/tf-cnn
train_features = data-tf/train
test_features = data-tf/test
dev_features = data-tf/dev
#directory where the language model will be retrieved
language = data-tf/lang
#directory where the language model will be retrieved that is used to create the decoding graph
language_test = data-tf/lang_test_bg
decode_num_jobs = 20
# ali label directory
alidir = tri3_ali
# the num job in ali dir 
num_ali_jobs = 30
# exp dir to get data
expdir = exp
# exp dir to store data
store_expdir = tf-exp

[nnet]
#name of the neural net
name = dnn
#name of the gmm model used for the alignments
gmm_name = tri3
# the type of lstm, can be 1,2,3 or other
lstm_type = 1
# the type of cnn, can be 1, 2 or other
cnn_type = -1
#size of the left and right context window
context_right = 7
context_left = 7
#number of neurons in the hidden layers
num_hidden_units = 1024
#number of hidden layers, only mean the FL hidden layers
num_hidden_layers = 0
#the network is initialized layer by layer. This parameters determines the frequency of adding layers. Adding will stop when the total number of layers is reached. Set to 0 if no layer-wise initialisation is required
add_layer_period = 0
#starting step, set to 'final' to skip nnet training
starting_step = 0
#if you're using monophone alignments, set to True
monophone = False
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = relu
#if you want to do l2 normalization after every layer set to 'True'
l2_norm = False
#if you want to use dropout set to a value smaller than 1
dropout = 1
#Flag for using batch normalisation
batch_norm = False
#number of passes over the entire database
num_epochs = 6
#determine which epoch we start to reset the acc threshold
acc_reset_epoch = 5
#initial learning rate of the neural net
initial_learning_rate = 0.001
#exponential weight decay parameter
learning_rate_decay = 1
#size of the minibatch (#utterances)
batch_size = 64
numutterances_per_minibatch = 2
#size of the validation set, set to 0 if you don't want to use one
valid_batches = 2
#frequency of evaluating the validation set
valid_frequency = 10
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
#number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 5
# a ACC threshold to lead the learning rate
acc_threshold = 0.01
#how many steps are taken between two checkpoints
check_freq = 10
#you can visualise the progress of the neural net with tensorboard
visualise = False

[cnn]
##--the first conv way--##
# the num of the conv layers
layers = 1
freq_dim = 40
# 15*3, use deltas feature
time_dim = 45
input_channel = 1
batch_norm = False
pool_size = 6
# for the freq conv, the input conv's in_channel should equal time_dim
conv0_kernel = [8, 1, 45, 150]
conv0_strides = [1, 2, 1, 1]

##--the second conv way--##
#layers = 2
#pool_size = 3
#freq_dim = 40
#time_dim = 15
#input_channel = 3
#batch_norm = False
# for the freq conv, the input conv's in_channel should equal time_dim
#conv0_kernel = [9, 9, 3, 256]
#conv0_strides = [4, 3, 256, 256]

[lstm]
# LayerNormBasicLSTMCell\CoupledInputForgetGateLSTMCell
cell_type = CoupledInputForgetGateLSTMCell
# here we prefer use fbank feature
freq_dim = 40
time_steps = 15
num_layers = 1
num_units = 256
keep_prob = 0.9
layer_norm = False
## tf-lstm
is_tf_lstm = False
# every step's size in frequency
tf_num_units = 64
feature_size = 15
frequency_skip = 1

[lstm2]
# LayerNormBasicLSTMCell\CoupledInputForgetGateLSTMCell
cell_type = LayerNormBasicLSTMCell
# here we prefer use fbank feature
freq_dim = 40
num_layers = 2
num_units = 832
num_proj = 512
keep_prob = 0.9
layer_norm = False
## tf-lstm
is_tf_lstm = False
# every step's size in frequency
tf_num_units = 64
feature_size = 8
frequency_skip = 1

[lstm3]
# LayerNormBasicLSTMCell\CoupledInputForgetGateLSTMCell
cell_type = LayerNormBasicLSTMCell
# here we prefer use fbank feature
freq_dim = 40
num_layers = 2
num_units = 832
num_proj = 512
time_steps = 20
keep_prob = 0.9
layer_norm = False
# whether we reuse the former sub-seq's output state
reuse_sub_seq_state = False
## tf-lstm
is_tf_lstm = False
# every step's size in frequency
tf_num_units = 64
feature_size = 8
frequency_skip = 1
